{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T0A6rTA6dMC"
      },
      "source": [
        "## Project ML4Graphs\n",
        "\n",
        "This project aims to implement Graph Attention Networks (GAT) for the master course on machine learning for graphs. GAT is a powerful graph neural network architecture that has shown promising results in various graph-related tasks, such as node classification and link prediction.\n",
        "\n",
        "The main objective of this project is to understand the underlying principles of GAT and its components, such as self-attention mechanisms and graph convolutional layers. By implementing GAT from scratch, we will gain a deeper understanding of how these components work together to capture complex relationships and patterns in graph-structured data.\n",
        "\n",
        "Furthermore, this project also focuses on improving upon the original GAT architecture. We will explore different variations and extensions of GAT, such as incorporating additional attention heads, introducing residual connections, or experimenting with different activation functions. Through these improvements, we aim to enhance the performance and generalization capabilities of GAT on various graph datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNBwyYqa6zDZ",
        "outputId": "da4b9e1a-429b-47a3-8ba9-c8e114a71fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n",
            "Collecting planetoids\n",
            "  Downloading planetoids-0.1-alpha.2.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from planetoids) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from planetoids) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from planetoids) (4.8.0.76)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from planetoids) (1.5.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from planetoids) (5.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from planetoids) (9.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from planetoids) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from planetoids) (1.11.4)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from planetoids) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from planetoids) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->planetoids) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->planetoids) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->planetoids) (8.2.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->planetoids) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->planetoids) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->planetoids) (1.16.0)\n",
            "Building wheels for collected packages: planetoids\n",
            "  Building wheel for planetoids (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for planetoids: filename=planetoids-0.1a2-py3-none-any.whl size=10698 sha256=37787868ae8681b724341ab65d5fb5c27d5036e979716c1d1d367eab9f354c2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/6f/b3/ab2e4b6285264b017f72b6614bff81d71e63ae7ca0a32a641f\n",
            "Successfully built planetoids\n",
            "Installing collected packages: planetoids\n",
            "Successfully installed planetoids-0.1a2\n",
            "Collecting labml_helpers\n",
            "  Downloading labml_helpers-0.4.89-py3-none-any.whl (24 kB)\n",
            "Collecting labml>=0.4.158 (from labml_helpers)\n",
            "  Downloading labml-0.4.168-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from labml_helpers) (2.1.0+cu121)\n",
            "Collecting gitpython (from labml>=0.4.158->labml_helpers)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from labml>=0.4.158->labml_helpers) (6.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from labml>=0.4.158->labml_helpers) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->labml_helpers) (2.1.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->labml>=0.4.158->labml_helpers)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->labml_helpers) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->labml_helpers) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml_helpers)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython, labml, labml_helpers\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.41 labml-0.4.168 labml_helpers-0.4.89 smmap-5.0.1\n"
          ]
        }
      ],
      "source": [
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  !pip install torch_geometric\n",
        "  !pip install planetoids\n",
        "  !pip install labml_helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the Necessary Libraries\n",
        "\n",
        "To begin our project, we need to import the necessary libraries. These libraries will provide us with the tools and functions we need to work with graphs and implement our machine learning models.\n",
        "\n",
        "In this project, we will be using the following libraries:\n",
        "\n",
        "- `torch`: A library for implementing machinea learning and deep learning\n",
        "- `torch_geometric`: A library for handling graph data and implementing graph neural networks.\n",
        "- `planetoids`: A library for loading and preprocessing graph datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yltRvsii6xPQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch_geometric.nn import MessagePassing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ-W6G_ADpSg"
      },
      "source": [
        "### Loading the Cora Dataset\n",
        "\n",
        "The Cora dataset is a popular graph dataset used in many graph machine learning tasks. It consists of scientific publications classified into one of seven classes. The citation network between the publications forms the graph structure. \n",
        "\n",
        "In this project, the Cora dataset is loaded using the `Planetoid` class from the PyTorch Geometric library. The dataset is stored in the specified path (`/home/cheremy/Documents/personal/ml4graphs/project/data`). The `DataLoader` class is used to create a generator that allows us to iterate over the dataset in batches of 32.\n",
        "\n",
        "### Model Parameters\n",
        "\n",
        "The model parameters are defined as follows:\n",
        "\n",
        "- `batch_size`: The size of the data batches. The model parameters are updated after each batch.\n",
        "\n",
        "- `nb_epochs`: The number of times the learning algorithm will work through the entire training dataset.\n",
        "\n",
        "- `patience`: The number of epochs to wait before stopping the training if the model performance does not improve.\n",
        "\n",
        "- `lr`: The learning rate for the optimizer. It determines the step size at each iteration while moving toward a minimum of a loss function.\n",
        "\n",
        "- `l2_coef`: The L2 regularization coefficient. It prevents the weights from growing too large, and helps to avoid overfitting.\n",
        "\n",
        "- `hid_units`: The number of hidden units per each attention head in each layer.\n",
        "\n",
        "- `n_heads`: The number of attention heads for each layer. An additional entry is added for the output layer.\n",
        "\n",
        "- `residual`: A boolean flag for using residual connections.\n",
        "\n",
        "- `DROPOUT_RATE`: The dropout rate for the dropout layer. It is a regularization technique where randomly selected neurons are ignored during training.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBXaCgRtDrgi",
        "outputId": "730ae2ae-41bc-4d17-cb0c-198ca992404b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Set the path where the dataset will be stored\n",
        "path = '/home/cheremy/Documents/personal/ml4graphs/project/data'\n",
        "\n",
        "# Load the Cora dataset\n",
        "dataset = Planetoid(root=path, name='Cora')\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# training params\n",
        "batch_size = 1\n",
        "nb_epochs = 10000\n",
        "patience = 100\n",
        "lr = 0.005  # learning rate\n",
        "l2_coef = 0.0005  # weight decay\n",
        "hid_units = [8] # numbers of hidden units per each attention head in each layer\n",
        "n_heads = [8, 1] # additional entry for the output layer\n",
        "residual = False\n",
        "\n",
        "DROPOUT_RATE: float = 0.5\n",
        "# Define the input, hidden, and output dimensions\n",
        "input_dim = dataset.num_features\n",
        "hidden_dim = 64\n",
        "output_dim = dataset.num_classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa7LFbeMDswa"
      },
      "source": [
        "### Graph Neural Network Implementation\n",
        "\n",
        "A Graph Neural Network (GNN) operates on a graph structure and updates the features of each node based on its neighbors. The core idea behind a GNN is to generate a node embedding by aggregating the embeddings of its neighboring nodes and its own initial embedding.\n",
        "\n",
        "The update rule for a node's feature in a GNN can be formulated as follows:\n",
        "\n",
        "h<sub>i</sub><sup>(l+1)</sup> = σ(Σ<sub>j∈N(i)</sub> A<sub>ij</sub> W<sup>(l)</sup> h<sub>j</sub><sup>(l)</sup>)\n",
        "\n",
        "where:\n",
        "\n",
        "- h<sub>i</sub><sup>(l+1)</sup> is the feature of node i at layer l+1.\n",
        "- σ is an activation function such as ReLU.\n",
        "- N(i) is the set of neighbors of node i.\n",
        "- A<sub>ij</sub> is the element at the i-th row and j-th column of the adjacency matrix A. It represents the edge weight between node i and node j.\n",
        "- W<sup>(l)</sup> is the weight matrix at layer l.\n",
        "- h<sub>j</sub><sup>(l)</sup> is the feature of node j at layer l.\n",
        "\n",
        "\n",
        "In this code, `in_features` and `out_features` are the dimensions of the input and output node features, respectively. The `reset_parameters` method initializes the weight matrix W using the Xavier uniform initialization. The `forward` method implements the update rule. It first computes the product of the adjacency matrix A and the product of the current node features h and the weight matrix W. Then, it applies the ReLU activation function to the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxvdIVj9Ay1E",
        "outputId": "7db7ba5e-5d27-4aa0-b7c4-fb7a3d94fa3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.94313645362854\n",
            "Epoch: 100, Loss: 0.05713015794754028\n",
            "Epoch: 200, Loss: 0.03638702630996704\n",
            "Epoch: 300, Loss: 0.028825655579566956\n",
            "Epoch: 400, Loss: 0.025052160024642944\n",
            "Epoch: 500, Loss: 0.022948285564780235\n",
            "Epoch: 600, Loss: 0.021747257560491562\n",
            "Epoch: 700, Loss: 0.02104821428656578\n",
            "Epoch: 800, Loss: 0.020640920847654343\n",
            "Epoch: 900, Loss: 0.020386312156915665\n",
            "Epoch: 1000, Loss: 0.0202333964407444\n",
            "Epoch: 1100, Loss: 0.020133282989263535\n",
            "Epoch: 1200, Loss: 0.020054256543517113\n",
            "Epoch: 1300, Loss: 0.019996710121631622\n",
            "Epoch: 1400, Loss: 0.01994277723133564\n",
            "Epoch: 1500, Loss: 0.019918160513043404\n",
            "Epoch: 1600, Loss: 0.019840063527226448\n",
            "Epoch: 1700, Loss: 0.019859356805682182\n",
            "Epoch: 1800, Loss: 0.01978999935090542\n",
            "Epoch: 1900, Loss: 0.019773637875914574\n",
            "Epoch: 2000, Loss: 0.019696002826094627\n",
            "Epoch: 2100, Loss: 0.019685735926032066\n",
            "Epoch: 2200, Loss: 0.019667670130729675\n",
            "Epoch: 2300, Loss: 0.019667766988277435\n",
            "Epoch: 2400, Loss: 0.019650645554065704\n",
            "Epoch: 2500, Loss: 0.019593125209212303\n",
            "Epoch: 2600, Loss: 0.019524898380041122\n",
            "Epoch: 2700, Loss: 0.019545966759324074\n",
            "Epoch: 2800, Loss: 0.019571367651224136\n",
            "Epoch: 2900, Loss: 0.019538937136530876\n",
            "Epoch: 3000, Loss: 0.01956672966480255\n",
            "Epoch: 3100, Loss: 0.01947879046201706\n",
            "Epoch: 3200, Loss: 0.019503045827150345\n",
            "Epoch: 3300, Loss: 0.01942150853574276\n",
            "Epoch: 3400, Loss: 0.019456548616290092\n",
            "Epoch: 3500, Loss: 0.019436687231063843\n",
            "Epoch: 3600, Loss: 0.019453058019280434\n",
            "Epoch: 3700, Loss: 0.019442059099674225\n",
            "Epoch: 3800, Loss: 0.019355809316039085\n",
            "Epoch: 3900, Loss: 0.019385088235139847\n",
            "Epoch: 4000, Loss: 0.019363874569535255\n",
            "Epoch: 4100, Loss: 0.019366782158613205\n",
            "Epoch: 4200, Loss: 0.019338712096214294\n",
            "Epoch: 4300, Loss: 0.019341830164194107\n",
            "Epoch: 4400, Loss: 0.019414927810430527\n",
            "Epoch: 4500, Loss: 0.019267966970801353\n",
            "Epoch: 4600, Loss: 0.019327104091644287\n",
            "Epoch: 4700, Loss: 0.019345199689269066\n",
            "Epoch: 4800, Loss: 0.019232552498579025\n",
            "Epoch: 4900, Loss: 0.01931673102080822\n",
            "Epoch: 5000, Loss: 0.019258704036474228\n",
            "Epoch: 5100, Loss: 0.019224336370825768\n",
            "Epoch: 5200, Loss: 0.0192378219217062\n",
            "Epoch: 5300, Loss: 0.01929665543138981\n",
            "Epoch: 5400, Loss: 0.019226279109716415\n",
            "Epoch: 5500, Loss: 0.01925460621714592\n",
            "Epoch: 5600, Loss: 0.01927325502038002\n",
            "Epoch: 5700, Loss: 0.01920618675649166\n",
            "Epoch: 5800, Loss: 0.019252382218837738\n",
            "Epoch: 5900, Loss: 0.019192922860383987\n",
            "Epoch: 6000, Loss: 0.019144603982567787\n",
            "Epoch: 6100, Loss: 0.019214313477277756\n",
            "Epoch: 6200, Loss: 0.019162626937031746\n",
            "Epoch: 6300, Loss: 0.019230278208851814\n",
            "Epoch: 6400, Loss: 0.019177023321390152\n",
            "Epoch: 6500, Loss: 0.019188575446605682\n",
            "Epoch: 6600, Loss: 0.019155582413077354\n",
            "Epoch: 6700, Loss: 0.019177662208676338\n",
            "Epoch: 6800, Loss: 0.019139336422085762\n",
            "Epoch: 6900, Loss: 0.01913481205701828\n",
            "Epoch: 7000, Loss: 0.019160887226462364\n",
            "Epoch: 7100, Loss: 0.019131245091557503\n",
            "Epoch: 7200, Loss: 0.01911318115890026\n",
            "Epoch: 7300, Loss: 0.019118165597319603\n",
            "Epoch: 7400, Loss: 0.019115786999464035\n",
            "Epoch: 7500, Loss: 0.019137172028422356\n",
            "Epoch: 7600, Loss: 0.019117049872875214\n",
            "Epoch: 7700, Loss: 0.019111856818199158\n",
            "Epoch: 7800, Loss: 0.0190846249461174\n",
            "Epoch: 7900, Loss: 0.01908102072775364\n",
            "Epoch: 8000, Loss: 0.019065557047724724\n",
            "Epoch: 8100, Loss: 0.019086990505456924\n",
            "Epoch: 8200, Loss: 0.01909779943525791\n",
            "Epoch: 8300, Loss: 0.019059909507632256\n",
            "Epoch: 8400, Loss: 0.019111139699816704\n",
            "Epoch: 8500, Loss: 0.019027229398489\n",
            "Epoch: 8600, Loss: 0.01904275454580784\n",
            "Epoch: 8700, Loss: 0.01906288042664528\n",
            "Epoch: 8800, Loss: 0.01907413639128208\n",
            "Epoch: 8900, Loss: 0.019031861796975136\n",
            "Epoch: 9000, Loss: 0.019053202122449875\n",
            "Epoch: 9100, Loss: 0.01902511715888977\n",
            "Epoch: 9200, Loss: 0.0190077256411314\n",
            "Epoch: 9300, Loss: 0.0190268624573946\n",
            "Epoch: 9400, Loss: 0.019032901152968407\n",
            "Epoch: 9500, Loss: 0.01903340220451355\n",
            "Epoch: 9600, Loss: 0.01907096989452839\n",
            "Epoch: 9700, Loss: 0.019007034599781036\n",
            "Epoch: 9800, Loss: 0.01903708279132843\n",
            "Epoch: 9900, Loss: 0.01901726797223091\n",
            "Predicted Labels: tensor([3, 4, 4,  ..., 3, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "class GNN(nn.Module):\n",
        "    ''' A simple fully connected neural network with one hidden layer '''\n",
        "\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index ):\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the SimpleModel\n",
        "model = GNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(nb_epochs):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x, y = data.x, data.y\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# After training, you can use the model for predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        x, y = data.x, data.y\n",
        "        output = model(x)\n",
        "        predicted_labels = torch.argmax(output, dim=1)\n",
        "        print(f\"Predicted Labels: {predicted_labels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RscZlGXADexZ"
      },
      "source": [
        "### Graph Convolutional Neural Network\n",
        "\n",
        "A Graph Convolutional Neural Network (GCN) is a type of Graph Neural Network that applies convolution operations on the graph structure. The convolution operation in a GCN is defined in the spectral domain of the graph, based on the graph Laplacian.\n",
        "\n",
        "The update rule for a node's feature in a GCN can be formulated as follows:\n",
        "\n",
        "h<sub>i</sub><sup>(l+1)</sup> = σ(D<sup>-1/2</sup> A D<sup>-1/2</sup> h<sub>i</sub><sup>(l)</sup> W<sup>(l)</sup>)\n",
        "\n",
        "where:\n",
        "\n",
        "- h<sub>i</sub><sup>(l+1)</sup> is the feature of node i at layer l+1.\n",
        "- σ is an activation function such as ReLU.\n",
        "- D is the degree matrix of the graph. It is a diagonal matrix where D<sub>ii</sub> is the degree of node i.\n",
        "- A is the adjacency matrix of the graph.\n",
        "- W<sup>(l)</sup> is the weight matrix at layer l.\n",
        "- h<sub>i</sub><sup>(l)</sup> is the feature of node i at layer l.\n",
        "\n",
        "The term D<sup>-1/2</sup> A D<sup>-1/2</sup> is the normalized adjacency matrix of the graph. It is used to account for the degree of each node during the convolution operation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQVvMnTlA2Cx",
        "outputId": "4e328345-d4c1-4b84-839f-c9a9ccf27c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.944478154182434\n",
            "Epoch: 100, Loss: 0.2618045210838318\n",
            "Epoch: 200, Loss: 0.16640669107437134\n",
            "Epoch: 300, Loss: 0.12378380447626114\n",
            "Epoch: 400, Loss: 0.10350265353918076\n",
            "Epoch: 500, Loss: 0.10627153515815735\n",
            "Epoch: 600, Loss: 0.08949212729930878\n",
            "Epoch: 700, Loss: 0.08288855850696564\n",
            "Epoch: 800, Loss: 0.07192418724298477\n",
            "Epoch: 900, Loss: 0.07015934586524963\n",
            "Epoch: 1000, Loss: 0.06260579824447632\n",
            "Epoch: 1100, Loss: 0.05933887138962746\n",
            "Epoch: 1200, Loss: 0.06044991314411163\n",
            "Epoch: 1300, Loss: 0.05376249924302101\n",
            "Epoch: 1400, Loss: 0.051890987902879715\n",
            "Epoch: 1500, Loss: 0.04898315295577049\n",
            "Epoch: 1600, Loss: 0.04601384326815605\n",
            "Epoch: 1700, Loss: 0.05400378629565239\n",
            "Epoch: 1800, Loss: 0.043475762009620667\n",
            "Epoch: 1900, Loss: 0.03856956958770752\n",
            "Epoch: 2000, Loss: 0.04094426706433296\n",
            "Epoch: 2100, Loss: 0.03703456372022629\n",
            "Epoch: 2200, Loss: 0.03715787082910538\n",
            "Epoch: 2300, Loss: 0.03457231819629669\n",
            "Epoch: 2400, Loss: 0.03992902487516403\n",
            "Epoch: 2500, Loss: 0.03280361741781235\n",
            "Epoch: 2600, Loss: 0.039001062512397766\n",
            "Epoch: 2700, Loss: 0.03848642110824585\n",
            "Epoch: 2800, Loss: 0.045910004526376724\n",
            "Epoch: 2900, Loss: 0.03374633193016052\n",
            "Epoch: 3000, Loss: 0.04187925532460213\n",
            "Epoch: 3100, Loss: 0.02956109680235386\n",
            "Epoch: 3200, Loss: 0.03188474476337433\n",
            "Epoch: 3300, Loss: 0.0354364849627018\n",
            "Epoch: 3400, Loss: 0.03420868515968323\n",
            "Epoch: 3500, Loss: 0.03245090693235397\n",
            "Epoch: 3600, Loss: 0.03761231526732445\n",
            "Epoch: 3700, Loss: 0.03223903104662895\n",
            "Epoch: 3800, Loss: 0.02832587994635105\n",
            "Epoch: 3900, Loss: 0.031633973121643066\n",
            "Epoch: 4000, Loss: 0.028627978637814522\n",
            "Epoch: 4100, Loss: 0.038169119507074356\n",
            "Epoch: 4200, Loss: 0.029713276773691177\n",
            "Epoch: 4300, Loss: 0.03453412279486656\n",
            "Epoch: 4400, Loss: 0.03034159168601036\n",
            "Epoch: 4500, Loss: 0.030441002920269966\n",
            "Epoch: 4600, Loss: 0.030811810865998268\n",
            "Epoch: 4700, Loss: 0.024843621999025345\n",
            "Epoch: 4800, Loss: 0.032043736428022385\n",
            "Epoch: 4900, Loss: 0.028767891228199005\n",
            "Epoch: 5000, Loss: 0.0284554585814476\n",
            "Epoch: 5100, Loss: 0.03534569591283798\n",
            "Epoch: 5200, Loss: 0.02875801920890808\n",
            "Epoch: 5300, Loss: 0.025622110813856125\n",
            "Epoch: 5400, Loss: 0.024594463407993317\n",
            "Epoch: 5500, Loss: 0.031883593648672104\n",
            "Epoch: 5600, Loss: 0.025301167741417885\n",
            "Epoch: 5700, Loss: 0.029967136681079865\n",
            "Epoch: 5800, Loss: 0.02338469587266445\n",
            "Epoch: 5900, Loss: 0.029730837792158127\n",
            "Epoch: 6000, Loss: 0.02904156595468521\n",
            "Epoch: 6100, Loss: 0.03271213173866272\n",
            "Epoch: 6200, Loss: 0.030811013653874397\n",
            "Epoch: 6300, Loss: 0.0380190871655941\n",
            "Epoch: 6400, Loss: 0.02919946238398552\n",
            "Epoch: 6500, Loss: 0.02925800159573555\n",
            "Epoch: 6600, Loss: 0.029084257781505585\n",
            "Epoch: 6700, Loss: 0.030369171872735023\n",
            "Epoch: 6800, Loss: 0.027995910495519638\n",
            "Epoch: 6900, Loss: 0.029567791149020195\n",
            "Epoch: 7000, Loss: 0.02654912695288658\n",
            "Epoch: 7100, Loss: 0.028277451172471046\n",
            "Epoch: 7200, Loss: 0.030725879594683647\n",
            "Epoch: 7300, Loss: 0.03150003403425217\n",
            "Epoch: 7400, Loss: 0.02399854175746441\n",
            "Epoch: 7500, Loss: 0.02183893695473671\n",
            "Epoch: 7600, Loss: 0.025259923189878464\n",
            "Epoch: 7700, Loss: 0.02330152317881584\n",
            "Epoch: 7800, Loss: 0.023024210706353188\n",
            "Epoch: 7900, Loss: 0.03112342394888401\n",
            "Epoch: 8000, Loss: 0.023227162659168243\n",
            "Epoch: 8100, Loss: 0.03138252720236778\n",
            "Epoch: 8200, Loss: 0.019759055227041245\n",
            "Epoch: 8300, Loss: 0.027579832822084427\n",
            "Epoch: 8400, Loss: 0.03337308019399643\n",
            "Epoch: 8500, Loss: 0.028041459619998932\n",
            "Epoch: 8600, Loss: 0.024455882608890533\n",
            "Epoch: 8700, Loss: 0.021553151309490204\n",
            "Epoch: 8800, Loss: 0.027397090569138527\n",
            "Epoch: 8900, Loss: 0.029613284394145012\n",
            "Epoch: 9000, Loss: 0.02068978361785412\n",
            "Epoch: 9100, Loss: 0.02483753114938736\n",
            "Epoch: 9200, Loss: 0.021399876102805138\n",
            "Epoch: 9300, Loss: 0.02632782980799675\n",
            "Epoch: 9400, Loss: 0.02422763779759407\n",
            "Epoch: 9500, Loss: 0.02414306439459324\n",
            "Epoch: 9600, Loss: 0.02534971386194229\n",
            "Epoch: 9700, Loss: 0.02092810906469822\n",
            "Epoch: 9800, Loss: 0.019333114847540855\n",
            "Epoch: 9900, Loss: 0.024595249444246292\n",
            "Predicted Labels: tensor([3, 4, 4,  ..., 3, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the GAT model\n",
        "model = GCN(dataset.num_node_features, dataset.num_classes)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(nb_epochs):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "        output = model.forward(x, edge_index)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# After training, you can use the model for predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "        output = model(x, edge_index)\n",
        "        predicted_labels = torch.argmax(output, dim=1)\n",
        "        print(f\"Predicted Labels: {predicted_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw88J-j0DxhX"
      },
      "source": [
        "### Graph Attention Network\n",
        "A Graph Attention Network (GAT) is a type of Graph Neural Network (GNN) that uses attention mechanisms to capture the importance of neighboring nodes when updating the features of a node in a graph.\n",
        "\n",
        "The update rule for a node's feature in a GAT can be formulated as follows:\n",
        "\n",
        "h<sub>i</sub><sup>(l+1)</sup> = σ(Σ<sub>j∈N(i)</sub> α<sub>ij</sub> W<sup>(l)</sup> h<sub>j</sub><sup>(l)</sup>)\n",
        "\n",
        "where:\n",
        "\n",
        "- h<sub>i</sub><sup>(l+1)</sup> is the feature of node i at layer l+1.\n",
        "- σ is an activation function such as ReLU.\n",
        "- N(i) is the set of neighbors of node i.\n",
        "- α<sub>ij</sub> is the attention coefficient that determines the importance of node j to node i.\n",
        "- W<sup>(l)</sup> is the weight matrix at layer l.\n",
        "- h<sub>j</sub><sup>(l)</sup> is the feature of node j at layer l.\n",
        "\n",
        "The attention coefficient α<sub>ij</sub> is computed using a shared attention mechanism that takes into account the features of both nodes i and j. It can be calculated as follows:\n",
        "\n",
        "α<sub>ij</sub> = softmax(LeakyReLU(a<sup>T</sup> [W<sup>(l)</sup> h<sub>i</sub><sup>(l)</sup> || W<sup>(l)</sup> h<sub>j</sub><sup>(l)</sup>]))\n",
        "\n",
        "where:\n",
        "\n",
        "- a is a learnable weight vector.\n",
        "- || denotes concatenation.\n",
        "- LeakyReLU is a leaky rectified linear unit activation function.\n",
        "\n",
        "By using attention mechanisms, GATs can effectively capture the importance of different nodes in a graph and adaptively aggregate information from neighboring nodes during the node feature update process. This allows GATs to achieve state-of-the-art performance on various graph-related tasks, such as node classification and link prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhC3qSwqKRh0",
        "outputId": "14fcafc4-022c-48cf-b5f9-5777cea126fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1.9394220113754272\n",
            "Epoch 10, Loss: 1.076474666595459\n",
            "Epoch 20, Loss: 0.555136501789093\n",
            "Epoch 30, Loss: 0.30722078680992126\n",
            "Epoch 40, Loss: 0.2492978274822235\n",
            "Epoch 50, Loss: 0.2130090743303299\n",
            "Epoch 60, Loss: 0.17001095414161682\n",
            "Epoch 70, Loss: 0.14431476593017578\n",
            "Epoch 80, Loss: 0.13524559140205383\n"
          ]
        }
      ],
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.concat = concat\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W)  # Linear transformation\n",
        "        N = h.size()[0]\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        a_input = torch.cat([Wh.repeat(1, N).view(N * N, -1), Wh.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = F.leaky_relu(torch.matmul(a_input, self.a).squeeze(2), negative_slope=self.alpha)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # Aggregation\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "        h_prime = F.elu(h_prime)  # Activation function\n",
        "\n",
        "        return h_prime\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_heads, dropout=0.6, alpha=0.2):\n",
        "        super(GAT, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # List of attention layers\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            GraphAttentionLayer(in_features, out_features, dropout, alpha) for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "    def forward(self, data):\n",
        "        h, edges_matrix = data.x, data.edge_index\n",
        "        # Construct the adjacency matrix\n",
        "        adjacency_matrix = torch.zeros(data.num_nodes, data.num_nodes)\n",
        "        for edge in zip(edges_matrix[0], edges_matrix[1]):\n",
        "            src, tgt = edge\n",
        "            adjacency_matrix[src, tgt] = 1\n",
        "\n",
        "        adjacency_matrix = adjacency_matrix.long()\n",
        "\n",
        "        # Move tensors to GPU device\n",
        "        h = h.to(device)\n",
        "        adjacency_matrix = adjacency_matrix.to(device)\n",
        "\n",
        "        # Stacking multiple attention heads\n",
        "        all_head_outputs = [layer(h, adjacency_matrix) for layer in self.attention_layers]\n",
        "        output = torch.mean(torch.stack(all_head_outputs), dim=0)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Create an instance of the GAT model\n",
        "model = GAT(in_features=dataset.num_features, out_features=dataset.num_classes, num_heads = 8)\n",
        "\n",
        "# Move the model to GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Move the input tensors to GPU device\n",
        "data = data.to(device)\n",
        "\n",
        "# Define a loss function and an optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 100\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    loss = criterion(output[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss for every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph Attention Network Version 2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GATv2(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
        "                 is_concat: bool = True,\n",
        "                 dropout: float = 0.6,\n",
        "                 leaky_relu_negative_slope: float = 0.2,\n",
        "                 share_weights: bool = False,\n",
        "                 flash = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.is_concat = is_concat\n",
        "        self.n_heads = n_heads\n",
        "        self.share_weights = share_weights\n",
        "\n",
        "        if is_concat:\n",
        "            assert out_features % n_heads == 0\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else:\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        self.linear_l = nn.ModuleList([nn.Linear(in_features, self.n_hidden, bias=False) for _ in range(n_heads)])\n",
        "        if share_weights:\n",
        "            self.linear_r = self.linear_l\n",
        "        else:\n",
        "            self.linear_r = nn.ModuleList([nn.Linear(in_features, self.n_hidden, bias=False) for _ in range(n_heads)])\n",
        "        self.attn = nn.Linear(self.n_hidden, 1, bias=False)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.flash = flash\n",
        "        if flash and not torch.backends.cuda.flash_sdp_enabled():\n",
        "            torch.backends.cuda.enable_flash_sdp(True)\n",
        "\n",
        "    def forward(self, data) -> torch.Tensor:\n",
        "        h, edges_matrix = data.x, data.edge_index\n",
        "        n_nodes = h.shape[0]\n",
        "\n",
        "        # Construct the adjacency matrix\n",
        "        adjacency_matrix = torch.zeros(data.num_nodes, data.num_nodes)\n",
        "        for edge in zip(edges_matrix[0], edges_matrix[1]):\n",
        "            src, tgt = edge\n",
        "            adjacency_matrix[src, tgt] = 1\n",
        "\n",
        "        adjacency_matrix = adjacency_matrix.long()\n",
        "\n",
        "        # Reshape adjacency matrix to match the shape of e\n",
        "        adjacency_matrix = adjacency_matrix.unsqueeze(2)\n",
        "\n",
        "        # Move tensors to GPU device\n",
        "        h = h.to(device)\n",
        "        adjacency_matrix = adjacency_matrix.to(device)\n",
        "\n",
        "        g_l = torch.stack([linear(h) for linear in self.linear_l], dim=1)\n",
        "        g_r = torch.stack([linear(h) for linear in self.linear_r], dim=1)\n",
        "\n",
        "        g_l_repeat = g_l.repeat(1, n_nodes, 1, 1)\n",
        "        g_r_repeat_interleave = g_r.repeat(1, 1, n_nodes, 1)\n",
        "\n",
        "        # Reshape g_l_repeat to match g_r_repeat_interleave\n",
        "        g_l_repeat = g_l_repeat.view(*g_r_repeat_interleave.shape)\n",
        "\n",
        "        g_sum = g_l_repeat + g_r_repeat_interleave\n",
        "        g_sum = g_sum.view(n_nodes, n_nodes, self.n_heads, self.n_hidden)\n",
        "\n",
        "        e = self.attn(self.activation(g_sum))\n",
        "        e = e.squeeze(-1)\n",
        "\n",
        "        assert adjacency_matrix.shape[0] == 1 or adjacency_matrix.shape[0] == n_nodes\n",
        "        assert adjacency_matrix.shape[1] == 1 or adjacency_matrix.shape[1] == n_nodes\n",
        "        assert adjacency_matrix.shape[2] == 1 or adjacency_matrix.shape[2] == self.n_heads\n",
        "        e = e.masked_fill(adjacency_matrix == 0, float('-inf'))\n",
        "\n",
        "        a = self.softmax(e)\n",
        "        a = self.dropout(a)\n",
        "\n",
        "        # Transpose the dimensions of 'a' to match with 'g_r' for matrix multiplication\n",
        "\n",
        "        attn_res = torch.einsum('ijh,jhf->ihf', a, g_r)\n",
        "\n",
        "        if self.is_concat:\n",
        "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
        "        else:\n",
        "            return attn_res.mean(dim=1)\n",
        "\n",
        "# Training loop\n",
        "model = GATv2(in_features=dataset.num_features, out_features=64, n_heads=8)\n",
        "\n",
        "# Move the model to GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay= 0.0005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)  # Move data to the appropriate device\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output[data.train_mask], y[data.train_mask])  # Use only the training mask\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "model.eval()\n",
        "\n",
        "data = dataset[0].to(device)  # Move data to the appropriate device for evaluation\n",
        "\n",
        "# Pass the features through the model for prediction\n",
        "pred = model(data).argmax(dim=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = (pred[data.test_mask] == y[data.test_mask]).sum()\n",
        "acc = int(correct) / int(data.test_mask.sum())\n",
        "print(f'Accuracy: {acc:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
